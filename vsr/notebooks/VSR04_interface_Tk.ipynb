{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5e4c0e-662f-4052-ad93-3d88a1d099a7",
   "metadata": {},
   "source": [
    "# Interface de test pour la reconnaissance visuelle de la parole (VSR)\n",
    "\n",
    "Cette application fournit une interface graphique permettant de tester un modèle de reconnaissance visuelle de la parole (VSR) qui reconnaît les mots \"oui\", \"non\", \"un\" et \"deux\" uniquement à partir des mouvements des lèvres.\n",
    "\n",
    "## Fonctionnalités\n",
    "\n",
    "- Affichage de la webcam en temps réel avec détection des lèvres\n",
    "- Capture d'une séquence d'images des lèvres pendant la prononciation d'un mot\n",
    "- Analyse de la séquence par un modèle CNN+LSTM préentraîné\n",
    "- Visualisation des images capturées et des transformations CNN\n",
    "- Affichage des probabilités de prédiction pour chaque mot\n",
    "\n",
    "## Prérequis\n",
    "\n",
    "- Python 3.x avec tkinter\n",
    "- OpenCV, dlib et imutils pour la détection faciale\n",
    "- TensorFlow pour le chargement et l'utilisation du modèle\n",
    "- Matplotlib et seaborn pour les visualisations\n",
    "- Le modèle préentraîné (`vsr_model_final.h5`) \n",
    "- Le détecteur de points faciaux (`shape_predictor_68_face_landmarks.dat`)\n",
    "\n",
    "## Utilisation\n",
    "\n",
    "1. Sélectionnez le mot que vous allez former avec vos lèvres\n",
    "2. Cliquez sur \"Enregistrer\" et attendez le décompte\n",
    "3. Formez le mot avec vos lèvres (sans parler) pendant l'enregistrement\n",
    "4. Examinez les résultats et évaluez la prédiction\n",
    "5. Consultez les statistiques pour analyser les performances globales\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a83a6e2d-78d3-4576-a377-c46b0044fbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import time\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "import dlib\n",
    "from imutils import face_utils\n",
    "from PIL import Image, ImageTk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3090ca6-4292-490f-a71f-9579fdcfcce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paramètres globaux\n",
    "img_size = 64  # Taille des images pour le modèle VSR\n",
    "max_sequence_length = 30  # Nombre max de frames pour VSR\n",
    "target_words = [\"oui\", \"non\", \"un\", \"deux\"]  # Mots cibles\n",
    "models_dir = \"../models\"  # Dossier contenant les modèles\n",
    "duration = 1  # Durée d'enregistrement en secondes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4344af23-6c86-4fe4-84ba-67601175d3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSRApp:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Test interactif de reconnaissance visuelle de la parole (VSR)\")\n",
    "        self.root.geometry(\"1200x800\")\n",
    "        \n",
    "        # Variables\n",
    "        self.is_recording = False\n",
    "        self.lip_frames = []\n",
    "        self.current_frame = None\n",
    "        self.video_capture = None\n",
    "        self.selected_word = tk.StringVar(value=target_words[0])\n",
    "        \n",
    "        # Chargement du modèle VSR\n",
    "        self.load_model()\n",
    "        \n",
    "        # Chargement du détecteur de visage\n",
    "        self.setup_face_detector()\n",
    "        \n",
    "        # Création de l'interface\n",
    "        self.create_widgets()\n",
    "        \n",
    "        # Démarrer la capture vidéo\n",
    "        self.start_video_capture()\n",
    "        \n",
    "        # Résultats des tests\n",
    "        self.test_results = []\n",
    "        \n",
    "    def load_model(self):\n",
    "        \"\"\"Charge le modèle VSR\"\"\"\n",
    "        try:\n",
    "            # Charger le modèle VSR\n",
    "            vsr_model_path = os.path.join(models_dir, 'vsr_model_final.h5')\n",
    "            if os.path.exists(vsr_model_path):\n",
    "                from tensorflow.keras.models import load_model\n",
    "                self.vsr_model = load_model(vsr_model_path)\n",
    "                print(\"Modèle VSR chargé avec succès\")\n",
    "            else:\n",
    "                print(f\"Modèle VSR non trouvé: {vsr_model_path}\")\n",
    "                self.vsr_model = None\n",
    "                \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Erreur\", f\"Erreur lors du chargement du modèle VSR: {e}\")\n",
    "            print(f\"Erreur lors du chargement du modèle VSR: {e}\")\n",
    "            self.vsr_model = None\n",
    "    \n",
    "    def setup_face_detector(self):\n",
    "        \"\"\"Configure le détecteur de visage pour VSR\"\"\"\n",
    "        try:\n",
    "            self.face_detector = dlib.get_frontal_face_detector()\n",
    "            predictor_path = os.path.join(models_dir, \"shape_predictor_68_face_landmarks.dat\")\n",
    "            if os.path.exists(predictor_path):\n",
    "                self.face_predictor = dlib.shape_predictor(predictor_path)\n",
    "                print(\"Détecteur de visage chargé avec succès\")\n",
    "            else:\n",
    "                print(f\"Détecteur de points faciaux non trouvé: {predictor_path}\")\n",
    "                self.face_predictor = None\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du chargement du détecteur de visage: {e}\")\n",
    "            self.face_detector = None\n",
    "            self.face_predictor = None\n",
    "    \n",
    "    def create_widgets(self):\n",
    "        \"\"\"Crée l'interface utilisateur\"\"\"\n",
    "        # Frame principale divisée en deux parties\n",
    "        main_frame = ttk.Frame(self.root)\n",
    "        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Partie gauche: Webcam et contrôles\n",
    "        left_frame = ttk.Frame(main_frame)\n",
    "        left_frame.pack(side=tk.LEFT, fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Cadre pour la webcam\n",
    "        self.webcam_frame = ttk.LabelFrame(left_frame, text=\"Webcam (détection des lèvres)\")\n",
    "        self.webcam_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        self.webcam_label = ttk.Label(self.webcam_frame)\n",
    "        self.webcam_label.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Contrôles\n",
    "        controls_frame = ttk.LabelFrame(left_frame, text=\"Contrôles\")\n",
    "        controls_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Sélection du mot\n",
    "        word_frame = ttk.Frame(controls_frame)\n",
    "        word_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        ttk.Label(word_frame, text=\"Mot à prononcer (sans parler):\").pack(side=tk.LEFT, padx=5)\n",
    "        word_combobox = ttk.Combobox(word_frame, textvariable=self.selected_word, values=target_words)\n",
    "        word_combobox.pack(side=tk.LEFT, padx=5, fill=tk.X, expand=True)\n",
    "        \n",
    "        # Bouton d'enregistrement\n",
    "        button_frame = ttk.Frame(controls_frame)\n",
    "        button_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        self.record_button = ttk.Button(button_frame, text=\"Enregistrer\", command=self.record)\n",
    "        self.record_button.pack(side=tk.LEFT, padx=5, fill=tk.X, expand=True)\n",
    "        \n",
    "        ttk.Button(button_frame, text=\"Voir résultats\", command=self.show_results).pack(\n",
    "            side=tk.LEFT, padx=5, fill=tk.X, expand=True)\n",
    "        \n",
    "        # Partie droite: Résultats et visualisations\n",
    "        right_frame = ttk.Frame(main_frame)\n",
    "        right_frame.pack(side=tk.RIGHT, fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Résultats de la prédiction\n",
    "        self.result_frame = ttk.LabelFrame(right_frame, text=\"Résultats\")\n",
    "        self.result_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        self.result_label = ttk.Label(self.result_frame, text=\"Formez un mot avec vos lèvres (sans parler)...\", font=(\"Arial\", 14))\n",
    "        self.result_label.pack(padx=10, pady=10)\n",
    "        \n",
    "        self.confidence_label = ttk.Label(self.result_frame, text=\"\")\n",
    "        self.confidence_label.pack(padx=10, pady=5)\n",
    "        \n",
    "        # Évaluation\n",
    "        eval_frame = ttk.Frame(self.result_frame)\n",
    "        eval_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        ttk.Button(eval_frame, text=\"Correct\", command=lambda: self.evaluate(True)).pack(\n",
    "            side=tk.LEFT, padx=5, fill=tk.X, expand=True)\n",
    "        \n",
    "        ttk.Button(eval_frame, text=\"Incorrect\", command=lambda: self.evaluate(False)).pack(\n",
    "            side=tk.LEFT, padx=5, fill=tk.X, expand=True)\n",
    "        \n",
    "        # Frame pour les visualisations\n",
    "        self.viz_frame = ttk.LabelFrame(right_frame, text=\"Visualisations\")\n",
    "        self.viz_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Tabs pour différentes visualisations\n",
    "        self.viz_tabs = ttk.Notebook(self.viz_frame)\n",
    "        self.viz_tabs.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Onglet séquence d'images\n",
    "        self.sequence_tab = ttk.Frame(self.viz_tabs)\n",
    "        self.viz_tabs.add(self.sequence_tab, text=\"Séquence d'images\")\n",
    "        \n",
    "        # Onglet transformations\n",
    "        self.transform_tab = ttk.Frame(self.viz_tabs)\n",
    "        self.viz_tabs.add(self.transform_tab, text=\"Transformations CNN\")\n",
    "        \n",
    "        # Onglet probabilités\n",
    "        self.proba_tab = ttk.Frame(self.viz_tabs)\n",
    "        self.viz_tabs.add(self.proba_tab, text=\"Probabilités\")\n",
    "        \n",
    "        # Barre de statut\n",
    "        self.status_bar = ttk.Label(self.root, text=\"Prêt\", relief=tk.SUNKEN, anchor=tk.W)\n",
    "        self.status_bar.pack(side=tk.BOTTOM, fill=tk.X)\n",
    "    \n",
    "    def start_video_capture(self):\n",
    "        \"\"\"Démarre la capture vidéo depuis la webcam\"\"\"\n",
    "        self.video_capture = cv2.VideoCapture(0)\n",
    "        if not self.video_capture.isOpened():\n",
    "            messagebox.showerror(\"Erreur\", \"Impossible d'accéder à la webcam\")\n",
    "            return\n",
    "            \n",
    "        self.update_webcam()\n",
    "    \n",
    "    def update_webcam(self):\n",
    "        \"\"\"Met à jour l'affichage de la webcam\"\"\"\n",
    "        ret, frame = self.video_capture.read()\n",
    "        if ret:\n",
    "            self.current_frame = frame.copy()\n",
    "            \n",
    "            # Si enregistrement vidéo en cours, traiter le frame pour VSR\n",
    "            if self.is_recording:\n",
    "                lip_region = self.extract_lip_region(frame)\n",
    "                if lip_region is not None:\n",
    "                    processed_lip = self.preprocess_lip_region(lip_region)\n",
    "                    if processed_lip is not None:\n",
    "                        self.lip_frames.append(processed_lip.squeeze())\n",
    "            \n",
    "            # Afficher les points du visage si disponible\n",
    "            if self.face_detector is not None and self.face_predictor is not None:\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                faces = self.face_detector(gray, 0)\n",
    "                for face in faces:\n",
    "                    shape = self.face_predictor(gray, face)\n",
    "                    shape = face_utils.shape_to_np(shape)\n",
    "                    \n",
    "                    # Dessiner les points des lèvres (48-68)\n",
    "                    for i in range(48, 68):\n",
    "                        cv2.circle(frame, (shape[i][0], shape[i][1]), 2, (0, 255, 0), -1)\n",
    "                    \n",
    "                    # Dessiner un rectangle autour des lèvres\n",
    "                    lips_points = shape[48:68]\n",
    "                    x, y = lips_points.min(axis=0)\n",
    "                    w, h = lips_points.max(axis=0) - lips_points.min(axis=0)\n",
    "                    margin = 10\n",
    "                    cv2.rectangle(frame, (x-margin, y-margin), (x+w+margin, y+h+margin), (0, 0, 255), 2)\n",
    "            \n",
    "            # Convertir pour affichage dans Tkinter\n",
    "            frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            img = Image.fromarray(frame_rgb)\n",
    "            imgtk = ImageTk.PhotoImage(image=img)\n",
    "            self.webcam_label.imgtk = imgtk\n",
    "            self.webcam_label.configure(image=imgtk)\n",
    "        \n",
    "        # Rappel pour mise à jour continue\n",
    "        self.root.after(10, self.update_webcam)\n",
    "    \n",
    "    def extract_lip_region(self, frame):\n",
    "        \"\"\"Extrait la région des lèvres d'une image\"\"\"\n",
    "        if self.face_detector is None or self.face_predictor is None:\n",
    "            return None\n",
    "            \n",
    "        # Convertir en niveaux de gris pour la détection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Détecter les visages\n",
    "        faces = self.face_detector(gray, 0)\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            return None\n",
    "        \n",
    "        # Prendre le premier visage détecté\n",
    "        face = faces[0]\n",
    "        \n",
    "        # Prédire les points de repère\n",
    "        shape = self.face_predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        \n",
    "        # Les points des lèvres sont les points 48-68 dans le modèle à 68 points\n",
    "        lips_points = shape[48:68]\n",
    "        \n",
    "        # Calculer le rectangle englobant pour les lèvres\n",
    "        x, y = lips_points.min(axis=0)\n",
    "        w, h = lips_points.max(axis=0) - lips_points.min(axis=0)\n",
    "        \n",
    "        # Ajouter une marge\n",
    "        margin = 10\n",
    "        x = max(0, x - margin)\n",
    "        y = max(0, y - margin)\n",
    "        w = min(frame.shape[1] - x, w + 2 * margin)\n",
    "        h = min(frame.shape[0] - y, h + 2 * margin)\n",
    "        \n",
    "        # Extraire la région des lèvres\n",
    "        lip_region = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        return lip_region\n",
    "    \n",
    "    def preprocess_lip_region(self, lip_region):\n",
    "        \"\"\"Prétraite une région de lèvres pour la prédiction\"\"\"\n",
    "        if lip_region is None:\n",
    "            return None\n",
    "        \n",
    "        # Convertir en niveaux de gris\n",
    "        gray = cv2.cvtColor(lip_region, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Redimensionner\n",
    "        resized = cv2.resize(gray, (img_size, img_size))\n",
    "        \n",
    "        # Normaliser\n",
    "        normalized = resized / 255.0\n",
    "        \n",
    "        # Ajouter les dimensions pour le modèle\n",
    "        processed = normalized.reshape(1, img_size, img_size, 1)\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def record(self):\n",
    "        \"\"\"Enregistre la vidéo pour la prédiction\"\"\"\n",
    "        # Vérifier que le modèle est chargé\n",
    "        if self.vsr_model is None or self.face_predictor is None:\n",
    "            messagebox.showerror(\"Erreur\", \"Le modèle VSR ou le détecteur de visage n'est pas chargé\")\n",
    "            return\n",
    "        \n",
    "        # Désactiver le bouton pendant l'enregistrement\n",
    "        self.record_button.config(state=tk.DISABLED)\n",
    "        self.status_bar.config(text=\"Préparation de l'enregistrement...\")\n",
    "        \n",
    "        # Lancer l'enregistrement dans un thread séparé\n",
    "        threading.Thread(target=self._record_thread).start()\n",
    "    \n",
    "    def _record_thread(self):\n",
    "        \"\"\"Fonction d'enregistrement exécutée dans un thread séparé\"\"\"\n",
    "        # Countdown avant enregistrement\n",
    "        for i in range(3, 0, -1):\n",
    "            self.status_bar.config(text=f\"Préparez-vous... {i}\")\n",
    "            time.sleep(1)\n",
    "        \n",
    "        self.status_bar.config(text=\"Formez le mot avec vos lèvres maintenant! (sans parler)\")\n",
    "        \n",
    "        # Démarrer l'enregistrement vidéo\n",
    "        self.is_recording = True\n",
    "        self.lip_frames = []\n",
    "        \n",
    "        # Attendre la durée d'enregistrement\n",
    "        time.sleep(duration)\n",
    "        \n",
    "        # Arrêter l'enregistrement vidéo\n",
    "        self.is_recording = False\n",
    "        \n",
    "        # Traiter les résultats\n",
    "        self.process_recording()\n",
    "        \n",
    "        # Réactiver le bouton\n",
    "        self.record_button.config(state=tk.NORMAL)\n",
    "    \n",
    "    def process_recording(self):\n",
    "        \"\"\"Traite l'enregistrement et affiche les résultats\"\"\"\n",
    "        self.status_bar.config(text=\"Traitement de l'enregistrement...\")\n",
    "        \n",
    "        # Vérifier s'il y a assez de frames\n",
    "        if len(self.lip_frames) < 5:\n",
    "            self.result_label.config(text=\"Pas assez d'images des lèvres détectées\")\n",
    "            self.confidence_label.config(text=\"Veuillez réessayer\")\n",
    "            self.status_bar.config(text=\"Prêt\")\n",
    "            return\n",
    "        \n",
    "        # Prédiction VSR\n",
    "        predicted_word, confidence, probabilities = self.predict_vsr()\n",
    "        \n",
    "        # Afficher les résultats\n",
    "        result_text = f\"Mot prédit: {predicted_word}\"\n",
    "        self.result_label.config(text=result_text)\n",
    "        self.confidence_label.config(text=f\"Confiance: {confidence:.2f}\")\n",
    "        \n",
    "        # Stocker le résultat temporaire\n",
    "        self.current_result = {\n",
    "            \"actual\": self.selected_word.get(),\n",
    "            \"predicted\": predicted_word,\n",
    "            \"confidence\": confidence,\n",
    "            \"probabilities\": probabilities.tolist()  # Pour la sauvegarde JSON\n",
    "        }\n",
    "        \n",
    "        # Afficher les visualisations\n",
    "        self.visualize_sequence()\n",
    "        self.visualize_probabilities(probabilities)\n",
    "        \n",
    "        self.status_bar.config(text=\"Prêt\")\n",
    "    \n",
    "    def predict_vsr(self):\n",
    "        \"\"\"Prédit le mot à partir de la séquence d'images des lèvres\"\"\"\n",
    "        try:\n",
    "            # Préparer la séquence pour le modèle\n",
    "            lip_sequence = self.prepare_lip_sequence()\n",
    "            \n",
    "            # Prédiction\n",
    "            prediction = self.vsr_model.predict(lip_sequence)\n",
    "            predicted_class = np.argmax(prediction[0])\n",
    "            confidence = prediction[0][predicted_class]\n",
    "            \n",
    "            # Résultat\n",
    "            predicted_word = target_words[predicted_class]\n",
    "            \n",
    "            return predicted_word, confidence, prediction[0]\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors de la prédiction VSR: {e}\")\n",
    "            return \"Erreur\", 0.0, np.zeros(len(target_words))\n",
    "    \n",
    "    def prepare_lip_sequence(self):\n",
    "        \"\"\"Prépare la séquence d'images des lèvres pour la prédiction\"\"\"\n",
    "        # Padding si nécessaire\n",
    "        sequence = self.lip_frames.copy()\n",
    "        if len(sequence) < max_sequence_length:\n",
    "            padding = [np.zeros((img_size, img_size)) for _ in range(max_sequence_length - len(sequence))]\n",
    "            sequence.extend(padding)\n",
    "        \n",
    "        # Limiter à max_sequence_length frames\n",
    "        sequence = sequence[:max_sequence_length]\n",
    "        \n",
    "        # Convertir en tableau numpy\n",
    "        return np.array(sequence).reshape(1, max_sequence_length, img_size, img_size, 1)\n",
    "    \n",
    "    def visualize_sequence(self):\n",
    "        \"\"\"Affiche la séquence d'images des lèvres\"\"\"\n",
    "        # Nettoyer les onglets\n",
    "        for widget in self.sequence_tab.winfo_children():\n",
    "            widget.destroy()\n",
    "        for widget in self.transform_tab.winfo_children():\n",
    "            widget.destroy()\n",
    "        \n",
    "        # Séquence d'images\n",
    "        fig_seq = plt.Figure(figsize=(10, 3))\n",
    "        \n",
    "        # Sélectionner quelques frames à afficher\n",
    "        num_display = min(6, len(self.lip_frames))\n",
    "        indices = np.linspace(0, len(self.lip_frames)-1, num_display, dtype=int)\n",
    "        \n",
    "        for i, idx in enumerate(indices):\n",
    "            ax = fig_seq.add_subplot(1, num_display, i+1)\n",
    "            ax.imshow(self.lip_frames[idx], cmap='gray')\n",
    "            ax.set_title(f\"Frame {idx+1}\")\n",
    "            ax.axis('off')\n",
    "        \n",
    "        fig_seq.tight_layout()\n",
    "        fig_seq.suptitle('Séquence d\\'images des lèvres', y=0.98)\n",
    "        \n",
    "        canvas_seq = FigureCanvasTkAgg(fig_seq, master=self.sequence_tab)\n",
    "        canvas_seq.draw()\n",
    "        canvas_seq.get_tk_widget().pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Visualisation des transformations (si modèle VSR disponible)\n",
    "        if self.vsr_model is not None and len(self.lip_frames) >= 5:\n",
    "            # Si possible, visualiser les premières couches convolutives\n",
    "            try:\n",
    "                from tensorflow.keras.models import Model\n",
    "                \n",
    "                # Créer un modèle pour extraire les caractéristiques\n",
    "                feature_model = Model(\n",
    "                    inputs=self.vsr_model.input,\n",
    "                    outputs=self.vsr_model.get_layer(index=1).output  # Première couche conv\n",
    "                )\n",
    "                \n",
    "                # Préparer la séquence\n",
    "                lip_sequence = self.prepare_lip_sequence()\n",
    "                \n",
    "                # Prédire pour obtenir les cartes de caractéristiques\n",
    "                features = feature_model.predict(lip_sequence)\n",
    "                \n",
    "                # Sélectionner la première frame\n",
    "                middle_frame_idx = 0\n",
    "                feature_maps = features[0][middle_frame_idx]\n",
    "                \n",
    "                # Afficher\n",
    "                fig_trans = plt.Figure(figsize=(10, 5))\n",
    "                \n",
    "                # Image originale\n",
    "                ax_orig = fig_trans.add_subplot(2, 4, 1)\n",
    "                ax_orig.imshow(self.lip_frames[0], cmap='gray')\n",
    "                ax_orig.set_title('Image originale')\n",
    "                ax_orig.axis('off')\n",
    "                \n",
    "                # Cartes de caractéristiques\n",
    "                num_features = min(7, feature_maps.shape[2])\n",
    "                for i in range(num_features):\n",
    "                    ax = fig_trans.add_subplot(2, 4, i+2)\n",
    "                    ax.imshow(feature_maps[:, :, i], cmap='viridis')\n",
    "                    ax.set_title(f'Filtre {i+1}')\n",
    "                    ax.axis('off')\n",
    "                \n",
    "                fig_trans.tight_layout()\n",
    "                fig_trans.suptitle('Transformations appliquées par la première couche CNN', y=0.98)\n",
    "                \n",
    "                canvas_trans = FigureCanvasTkAgg(fig_trans, master=self.transform_tab)\n",
    "                canvas_trans.draw()\n",
    "                canvas_trans.get_tk_widget().pack(fill=tk.BOTH, expand=True)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Erreur lors de la visualisation des transformations: {e}\")\n",
    "                error_label = ttk.Label(self.transform_tab, text=f\"Erreur: {e}\")\n",
    "                error_label.pack(padx=10, pady=10)\n",
    "    \n",
    "    def visualize_probabilities(self, probabilities):\n",
    "        \"\"\"Affiche les probabilités pour chaque classe\"\"\"\n",
    "        # Nettoyer l'onglet\n",
    "        for widget in self.proba_tab.winfo_children():\n",
    "            widget.destroy()\n",
    "        \n",
    "        # Créer le graphique\n",
    "        fig = plt.Figure(figsize=(8, 4))\n",
    "        ax = fig.add_subplot(111)\n",
    "        \n",
    "        bars = ax.bar(target_words, probabilities, color='skyblue')\n",
    "        \n",
    "        # Mettre en évidence la classe prédite\n",
    "        predicted_class = np.argmax(probabilities)\n",
    "        bars[predicted_class].set_color('navy')\n",
    "        \n",
    "        # Ajouter les valeurs sur les barres\n",
    "        for i, p in enumerate(probabilities):\n",
    "            ax.text(i, p + 0.01, f'{p:.3f}', ha='center')\n",
    "        \n",
    "        ax.set_ylim(0, 1.1)\n",
    "        ax.set_ylabel('Probabilité')\n",
    "        ax.set_title('Probabilités pour chaque mot')\n",
    "        \n",
    "        # Ajouter au canvas\n",
    "        canvas = FigureCanvasTkAgg(fig, master=self.proba_tab)\n",
    "        canvas.draw()\n",
    "        canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)\n",
    "    \n",
    "    def evaluate(self, is_correct):\n",
    "        \"\"\"Enregistre l'évaluation de l'utilisateur\"\"\"\n",
    "        if hasattr(self, 'current_result'):\n",
    "            # Ajouter le résultat aux résultats des tests\n",
    "            self.current_result[\"correct\"] = is_correct\n",
    "            self.test_results.append(self.current_result)\n",
    "            \n",
    "            # Message de confirmation\n",
    "            self.status_bar.config(text=f\"Résultat enregistré. Total: {len(self.test_results)} tests.\")\n",
    "            \n",
    "            # Effacer le résultat actuel\n",
    "            delattr(self, 'current_result')\n",
    "        else:\n",
    "            messagebox.showinfo(\"Info\", \"Aucun résultat à évaluer. Veuillez d'abord enregistrer un mot.\")\n",
    "    \n",
    "    def show_results(self):\n",
    "        \"\"\"Affiche les résultats des tests\"\"\"\n",
    "        if not self.test_results:\n",
    "            messagebox.showinfo(\"Info\", \"Aucun résultat enregistré\")\n",
    "            return\n",
    "        \n",
    "        # Créer une nouvelle fenêtre pour les résultats\n",
    "        results_window = tk.Toplevel(self.root)\n",
    "        results_window.title(\"Résultats des tests VSR\")\n",
    "        results_window.geometry(\"800x600\")\n",
    "        \n",
    "        # Frame principale\n",
    "        main_frame = ttk.Frame(results_window)\n",
    "        main_frame.pack(fill=tk.BOTH, expand=True, padx=10, pady=10)\n",
    "        \n",
    "        # Tableau des résultats\n",
    "        table_frame = ttk.LabelFrame(main_frame, text=\"Résultats détaillés\")\n",
    "        table_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        # Treeview pour afficher les résultats\n",
    "        columns = (\"actual\", \"predicted\", \"confidence\", \"correct\")\n",
    "        tree = ttk.Treeview(table_frame, columns=columns, show=\"headings\")\n",
    "        \n",
    "        # Définir les en-têtes\n",
    "        tree.heading(\"actual\", text=\"Mot formé\")\n",
    "        tree.heading(\"predicted\", text=\"Mot prédit\")\n",
    "        tree.heading(\"confidence\", text=\"Confiance\")\n",
    "        tree.heading(\"correct\", text=\"Correct\")\n",
    "        \n",
    "        # Définir les largeurs de colonnes\n",
    "        tree.column(\"actual\", width=150)\n",
    "        tree.column(\"predicted\", width=150)\n",
    "        tree.column(\"confidence\", width=100)\n",
    "        tree.column(\"correct\", width=100)\n",
    "        \n",
    "        # Ajouter les données\n",
    "        for i, result in enumerate(self.test_results):\n",
    "            tree.insert(\"\", tk.END, values=(\n",
    "                result[\"actual\"],\n",
    "                result[\"predicted\"],\n",
    "                f\"{result['confidence']:.2f}\",\n",
    "                \"Oui\" if result[\"correct\"] else \"Non\"\n",
    "            ))\n",
    "        \n",
    "        # Ajouter scrollbar\n",
    "        scrollbar = ttk.Scrollbar(table_frame, orient=tk.VERTICAL, command=tree.yview)\n",
    "        tree.configure(yscroll=scrollbar.set)\n",
    "        scrollbar.pack(side=tk.RIGHT, fill=tk.Y)\n",
    "        tree.pack(fill=tk.BOTH, expand=True)\n",
    "        \n",
    "        # Statistiques\n",
    "        stats_frame = ttk.LabelFrame(main_frame, text=\"Statistiques\")\n",
    "        stats_frame.pack(fill=tk.X, padx=5, pady=5)\n",
    "        \n",
    "        # Calculer l'exactitude\n",
    "        correct_count = sum(1 for r in self.test_results if r[\"correct\"])\n",
    "        accuracy = correct_count / len(self.test_results) if self.test_results else 0\n",
    "        \n",
    "        # Calculer les statistiques par mot\n",
    "        word_stats = {}\n",
    "        for word in target_words:\n",
    "            word_results = [r for r in self.test_results if r[\"actual\"] == word]\n",
    "            if word_results:\n",
    "                word_correct = sum(1 for r in word_results if r[\"correct\"])\n",
    "                word_accuracy = word_correct / len(word_results)\n",
    "                word_stats[word] = {\n",
    "                    \"count\": len(word_results),\n",
    "                    \"correct\": word_correct,\n",
    "                    \"accuracy\": word_accuracy\n",
    "                }\n",
    "        \n",
    "        # Afficher les statistiques\n",
    "        ttk.Label(stats_frame, text=f\"Exactitude globale: {accuracy:.2%} ({correct_count}/{len(self.test_results)})\").pack(anchor=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        for word, stats in word_stats.items():\n",
    "            ttk.Label(stats_frame, text=f\"Mot '{word}': {stats['accuracy']:.2%} ({stats['correct']}/{stats['count']})\").pack(anchor=tk.W, padx=5, pady=2)\n",
    "        \n",
    "        # Matrice de confusion\n",
    "        viz_frame = ttk.LabelFrame(main_frame, text=\"Matrice de confusion\")\n",
    "        viz_frame.pack(fill=tk.BOTH, expand=True, padx=5, pady=5)\n",
    "        \n",
    "        try:\n",
    "            from sklearn.metrics import confusion_matrix\n",
    "            import seaborn as sns\n",
    "            \n",
    "            # Collecter les étiquettes réelles et prédites\n",
    "            y_true = []\n",
    "            y_pred = []\n",
    "            \n",
    "            for result in self.test_results:\n",
    "                if result[\"actual\"] in target_words and result[\"predicted\"] in target_words:\n",
    "                    y_true.append(target_words.index(result[\"actual\"]))\n",
    "                    y_pred.append(target_words.index(result[\"predicted\"]))\n",
    "            \n",
    "            if len(y_true) > 0:\n",
    "                # Calculer la matrice de confusion\n",
    "                cm = confusion_matrix(y_true, y_pred, labels=range(len(target_words)))\n",
    "                \n",
    "                # Afficher la matrice\n",
    "                fig = plt.Figure(figsize=(6, 4))\n",
    "                ax = fig.add_subplot(111)\n",
    "                sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=target_words, yticklabels=target_words, ax=ax)\n",
    "                ax.set_xlabel('Prédit')\n",
    "                ax.set_ylabel('Réel')\n",
    "                ax.set_title('Matrice de confusion')\n",
    "                \n",
    "                canvas = FigureCanvasTkAgg(fig, master=viz_frame)\n",
    "                canvas.draw()\n",
    "                canvas.get_tk_widget().pack(fill=tk.BOTH, expand=True)\n",
    "            else:\n",
    "                ttk.Label(viz_frame, text=\"Pas assez de données pour afficher la matrice de confusion\").pack(padx=10, pady=10)\n",
    "                \n",
    "        except Exception as e:\n",
    "            ttk.Label(viz_frame, text=f\"Erreur lors de la création de la matrice de confusion: {e}\").pack(padx=10, pady=10)\n",
    "        \n",
    "        # Bouton pour sauvegarder les résultats\n",
    "        ttk.Button(main_frame, text=\"Sauvegarder les résultats\", \n",
    "                  command=lambda: self.save_results(results_window)).pack(pady=10)\n",
    "    \n",
    "    def save_results(self, parent_window):\n",
    "        \"\"\"Sauvegarde les résultats des tests dans un fichier\"\"\"\n",
    "        try:\n",
    "            import json\n",
    "            from datetime import datetime\n",
    "            \n",
    "            # Générer un nom de fichier avec la date et l'heure\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"vsr_test_results_{timestamp}.json\"\n",
    "            \n",
    "            # Sauvegarder en JSON\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(self.test_results, f, ensure_ascii=False, indent=4)\n",
    "            \n",
    "            messagebox.showinfo(\"Succès\", f\"Résultats sauvegardés dans {filename}\", parent=parent_window)\n",
    "            \n",
    "        except Exception as e:\n",
    "            messagebox.showerror(\"Erreur\", f\"Erreur lors de la sauvegarde: {e}\", parent=parent_window)\n",
    "    \n",
    "    def on_closing(self):\n",
    "        \"\"\"Gestion de la fermeture de l'application\"\"\"\n",
    "        if self.video_capture is not None:\n",
    "            self.video_capture.release()\n",
    "        self.root.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12b560da-e3cd-4965-b0b9-4317835b70a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modèle VSR chargé avec succès\n",
      "Détecteur de visage chargé avec succès\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step   \n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "WARNING:tensorflow:5 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021666FBDAB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 8 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021666FBDAB0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 133ms/step\n"
     ]
    }
   ],
   "source": [
    "# Point d'entrée principal\n",
    "if __name__ == \"__main__\":\n",
    "    root = tk.Tk()\n",
    "    app = VSRApp(root)\n",
    "    root.protocol(\"WM_DELETE_WINDOW\", app.on_closing)\n",
    "    root.mainloop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1437c08e-0e51-47f7-85b2-ad7822272fca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
