{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86b2390e-daf8-402a-88c6-18b45b0fa3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.10.6)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Détecteur de points de repère chargé !\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import face_utils\n",
    "import os\n",
    "import time\n",
    "import tkinter as tk\n",
    "from tkinter import ttk, messagebox\n",
    "from PIL import Image, ImageTk\n",
    "import threading\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import pygame\n",
    "import tempfile\n",
    "\n",
    "# Configuration des chemins\n",
    "video_dir = \"../data/video_recordings\"\n",
    "lip_dir = \"../data/lip_frames\"\n",
    "audio_dir = \"../data/audio_recordings\"  # Nouveau dossier pour les fichiers audio\n",
    "\n",
    "# Mots cibles à enregistrer\n",
    "target_words = [\"oui\", \"non\", \"un\", \"deux\"]\n",
    "\n",
    "# Nombre d'exemples par mot\n",
    "examples_per_word = 10\n",
    "\n",
    "# Paramètres d'enregistrement\n",
    "frame_rate = 30    # Images par seconde\n",
    "duration = 1       # Durée d'enregistrement en secondes (1 seconde suffit pour les mots courts)\n",
    "width = 640        # Largeur de la vidéo\n",
    "height = 480       # Hauteur de la vidéo\n",
    "audio_fs = 16000   # Fréquence d'échantillonnage audio (Hz)\n",
    "\n",
    "# Créer les dossiers s'ils n'existent pas\n",
    "for word in target_words:\n",
    "    word_video_dir = os.path.join(video_dir, word)\n",
    "    word_lip_dir = os.path.join(lip_dir, word)\n",
    "    word_audio_dir = os.path.join(audio_dir, word)\n",
    "    os.makedirs(word_video_dir, exist_ok=True)\n",
    "    os.makedirs(word_lip_dir, exist_ok=True)\n",
    "    os.makedirs(word_audio_dir, exist_ok=True)\n",
    "\n",
    "# Initialisation du détecteur de visage et du prédicteur de points de repère\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor_path = \"../models/shape_predictor_68_face_landmarks.dat\"\n",
    "if os.path.exists(predictor_path):\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "    print(\"Détecteur de points de repère chargé !\")\n",
    "else:\n",
    "    print(f\"ATTENTION: Le fichier {predictor_path} n'a pas été trouvé.\")\n",
    "    print(\"Téléchargez-le depuis http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6274183e-8df8-4374-973f-4043b2c9ad02",
   "metadata": {},
   "source": [
    "# Détection et extraction des régions des lèvres pour la VSR\n",
    "\n",
    "La détection et l'extraction des régions des lèvres constituent la base technique de tout système de reconnaissance visuelle de la parole (VSR). Ce document détaille l'implémentation et le fonctionnement de ces processus dans le code.\n",
    "\n",
    "## Détection des points du visage\n",
    "\n",
    "```python\n",
    "# Initialisation du détecteur de visage et du prédicteur de points de repère\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor_path = \"../models/shape_predictor_68_face_landmarks.dat\"\n",
    "if os.path.exists(predictor_path):\n",
    "    predictor = dlib.shape_predictor(predictor_path)\n",
    "```\n",
    "\n",
    "Le système repose sur deux composants principaux de dlib:\n",
    "1. **Détecteur de visage frontal**: Identifie les visages dans l'image\n",
    "2. **Prédicteur de points de repère**: Modèle pré-entraîné qui localise 68 points caractéristiques du visage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de35a3bc-b9aa-4b2b-92e0-b952141fe02c",
   "metadata": {},
   "source": [
    "## Fonctionnement du shape_predictor\n",
    "\n",
    "Le modèle `shape_predictor_68_face_landmarks.dat` est basé sur une architecture d'apprentissage \"Ensemble of Regression Trees\" (ERT). Ce modèle a été entraîné sur des milliers d'images de visages annotées manuellement et peut prédire la position des 68 points faciaux avec grande précision, même dans des conditions variables.\n",
    "\n",
    "## Les 68 points faciaux et leur signification\n",
    "\n",
    "Le modèle utilise une convention standard de 68 points disposés ainsi:\n",
    "* Points 1-17 : Contour du visage\n",
    "* Points 18-22 : Sourcil gauche\n",
    "* Points 23-27 : Sourcil droit\n",
    "* Points 28-36 : Nez\n",
    "* Points 37-42 : Œil gauche\n",
    "* Points 43-48 : Œil droit\n",
    "* **Points 49-68 : Région buccale**\n",
    "\n",
    "Pour la VSR, nous utilisons spécifiquement:\n",
    "* **Points 49-60 : Contour extérieur des lèvres**\n",
    "* **Points 61-68 : Contour intérieur des lèvres**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a46b8ce6-300b-4e9c-a772-c007db5f48bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialisation de pygame pour la lecture audio\n",
    "pygame.mixer.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dfcae4b-1637-420f-a7f5-9329bb4e261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_lip_region(frame):\n",
    "    \"\"\"\n",
    "    Extrait la région des lèvres d'une image.\n",
    "    \n",
    "    Args:\n",
    "        frame: Image à analyser\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (region des lèvres, points des lèvres, visage détecté)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convertir en niveaux de gris pour la détection\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Détecter les visages\n",
    "        faces = detector(gray, 0)\n",
    "        \n",
    "        if len(faces) == 0:\n",
    "            return None, None, None\n",
    "        \n",
    "        # Prendre le premier visage détecté\n",
    "        face = faces[0]\n",
    "        \n",
    "        # Prédire les points de repère\n",
    "        shape = predictor(gray, face)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "        \n",
    "        # Les points des lèvres sont les points 48-68 dans le modèle à 68 points\n",
    "        lips_points = shape[48:68]\n",
    "        \n",
    "        # Calculer le rectangle englobant pour les lèvres\n",
    "        x, y = lips_points.min(axis=0)\n",
    "        w, h = lips_points.max(axis=0) - lips_points.min(axis=0)\n",
    "        \n",
    "        # Ajouter une marge\n",
    "        margin = 10\n",
    "        x = max(0, x - margin)\n",
    "        y = max(0, y - margin)\n",
    "        w = min(frame.shape[1] - x, w + 2 * margin)\n",
    "        h = min(frame.shape[0] - y, h + 2 * margin)\n",
    "        \n",
    "        # Extraire la région des lèvres\n",
    "        lip_region = frame[y:y+h, x:x+w]\n",
    "        \n",
    "        return lip_region, lips_points, face\n",
    "    except Exception as e:\n",
    "        print(f\"Erreur lors de l'extraction des lèvres: {e}\")\n",
    "        return None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e4c29a-bb8c-493c-8f0a-cc27403ea955",
   "metadata": {},
   "source": [
    "## Analyse étape par étape\n",
    "\n",
    "### 1. Préparation de l'image\n",
    "```python\n",
    "gray = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "```\n",
    "Cette conversion transforme l'image du format BGR (OpenCV) en niveaux de gris.\n",
    "\n",
    "### 2. Détection du visage\n",
    "```python\n",
    "faces = detector(gray, 0)\n",
    "if len(faces) == 0:\n",
    "    return None, None, None\n",
    "face = faces[0]\n",
    "```\n",
    "Le détecteur utilise un algorithme HOG+SVM pour localiser les visages. Le paramètre `0` indique qu'aucune pyramide d'image n'est utilisée (pas d'upsampling). Si aucun visage n'est détecté, la fonction retourne immédiatement `None`.\n",
    "\n",
    "### 3. Détection des points de repère\n",
    "```python\n",
    "shape = predictor(gray, face)\n",
    "shape = face_utils.shape_to_np(shape)\n",
    "```\n",
    "Le prédicteur analyse la région faciale et génère 68 points. La fonction `shape_to_np` convertit l'objet dlib en tableau NumPy.\n",
    "\n",
    "### 4. Isolation des points des lèvres\n",
    "```python\n",
    "lips_points = shape[48:68]\n",
    "```\n",
    "Seuls les 20 points correspondant aux lèvres sont conservés.\n",
    "\n",
    "### 5. Calcul du rectangle englobant\n",
    "```python\n",
    "x, y = lips_points.min(axis=0)\n",
    "w, h = lips_points.max(axis=0) - lips_points.min(axis=0)\n",
    "```\n",
    "Cette opération trouve les coordonnées extrêmes des points pour créer un rectangle contenant tous les points des lèvres.\n",
    "\n",
    "### 6. Ajout d'une marge\n",
    "```python\n",
    "margin = 10\n",
    "x = max(0, x - margin)\n",
    "y = max(0, y - margin)\n",
    "w = min(frame.shape[1] - x, w + 2 * margin)\n",
    "h = min(frame.shape[0] - y, h + 2 * margin)\n",
    "```\n",
    "Une marge de 10 pixels est ajoutée autour des lèvres pour s'assurer que toute la région est capturée. Les conditions `max()` et `min()` garantissent que le rectangle reste dans les limites de l'image.\n",
    "\n",
    "### 7. Extraction de la région\n",
    "```python\n",
    "lip_region = frame[y:y+h, x:x+w]\n",
    "```\n",
    "Cette opération crée une sous-image contenant uniquement la région des lèvres.\n",
    "\n",
    "## Visualisation des points des lèvres\n",
    "\n",
    "Pour le retour visuel pendant la capture, nous dessinons les points détectés:\n",
    "\n",
    "```python\n",
    "if face is not None and lips_points is not None:\n",
    "    # Dessiner les points des lèvres\n",
    "    for (x, y) in lips_points:\n",
    "        cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)\n",
    "```\n",
    "\n",
    "Cela permet de confirmer visuellement que la détection fonctionne correctement.\n",
    "\n",
    "## Évaluation de la qualité de détection\n",
    "\n",
    "Le système évalue la qualité de l'enregistrement en calculant le pourcentage de frames où les lèvres ont été correctement détectées:\n",
    "\n",
    "```python\n",
    "if len(self.lip_regions) < len(self.frames) * 0.7:\n",
    "    self.video_quality.set(\"Mauvais: Lèvres détectées dans moins de 70% des images\")\n",
    "elif len(self.lip_regions) < len(self.frames) * 0.9:\n",
    "    self.video_quality.set(\"Acceptable: Lèvres détectées dans plus de 70% des images\")\n",
    "else:\n",
    "    self.video_quality.set(\"Excellent: Lèvres détectées dans plus de 90% des images\")\n",
    "```\n",
    "\n",
    "\n",
    "## Prétraitement additionnel pour la VSR\n",
    "\n",
    "Dans un système VSR complet, les régions extraites subissent généralement des traitements supplémentaires:\n",
    "\n",
    "1. **Redimensionnement**: Normalisation à une taille standard (par exemple 64×64 pixels)\n",
    "2. **Normalisation d'intensité**: Ajustement des niveaux de luminosité et contraste\n",
    "3. **Conversion en niveaux de gris**: Réduction de la dimensionnalité\n",
    "4. **Augmentation de données**: Rotation, mise à l'échelle, et variations d'intensité pour améliorer la robustesse\n",
    "\n",
    "Ces étapes seront incorporées dans la phase d'entraînement du modèle VSR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aafc987b-d9c4-472b-90e1-2c2686a92396",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VSRDataCollector:\n",
    "    def __init__(self, root):\n",
    "        self.root = root\n",
    "        self.root.title(\"Collecteur de données VSR\")\n",
    "        self.root.geometry(\"1000x700\")\n",
    "        self.root.resizable(True, True)\n",
    "        \n",
    "        # Variables\n",
    "        self.cap = None\n",
    "        self.is_recording = False\n",
    "        self.frames = []\n",
    "        self.lip_regions = []\n",
    "        self.audio_data = None\n",
    "        self.current_word = tk.StringVar(value=target_words[0])\n",
    "        self.current_example = tk.IntVar(value=1)\n",
    "        self.countdown_value = tk.IntVar(value=3)\n",
    "        self.progress = tk.DoubleVar(value=0)\n",
    "        self.recording_thread = None\n",
    "        self.video_quality = tk.StringVar(value=\"\")\n",
    "        self.temp_audio_file = None\n",
    "        self.temp_video_file = None\n",
    "        self.status_message = tk.StringVar(value=\"\")  # Nouvelle variable pour les messages\n",
    "        \n",
    "        # Configurer l'interface\n",
    "        self.setup_ui()\n",
    "        \n",
    "        # Démarrer la webcam\n",
    "        self.start_webcam()\n",
    "        \n",
    "        # Configurer la fermeture propre\n",
    "        self.root.protocol(\"WM_DELETE_WINDOW\", self.on_closing)\n",
    "    \n",
    "    def setup_ui(self):\n",
    "        # Créer une structure de grille\n",
    "        main_frame = ttk.Frame(self.root, padding=10)\n",
    "        main_frame.grid(row=0, column=0, sticky=\"nsew\")\n",
    "        \n",
    "        # Configurer les poids des lignes et colonnes\n",
    "        self.root.columnconfigure(0, weight=1)\n",
    "        self.root.rowconfigure(0, weight=1)\n",
    "        main_frame.columnconfigure(0, weight=2)\n",
    "        main_frame.columnconfigure(1, weight=1)\n",
    "        main_frame.rowconfigure(0, weight=1)\n",
    "        \n",
    "        # Créer les deux panneaux principaux\n",
    "        left_panel = ttk.Frame(main_frame, padding=5)\n",
    "        left_panel.grid(row=0, column=0, sticky=\"nsew\")\n",
    "        \n",
    "        right_panel = ttk.Frame(main_frame, padding=5)\n",
    "        right_panel.grid(row=0, column=1, sticky=\"nsew\")\n",
    "        \n",
    "        # Configurer le panneau gauche (vidéo)\n",
    "        left_panel.columnconfigure(0, weight=1)\n",
    "        left_panel.rowconfigure(0, weight=1)\n",
    "        left_panel.rowconfigure(1, weight=0)\n",
    "        \n",
    "        # Webcam view\n",
    "        self.webcam_label = ttk.Label(left_panel, borderwidth=2, relief=\"solid\")\n",
    "        self.webcam_label.grid(row=0, column=0, sticky=\"nsew\", padx=5, pady=5)\n",
    "        \n",
    "        # Barre de progression\n",
    "        progress_frame = ttk.Frame(left_panel)\n",
    "        progress_frame.grid(row=1, column=0, sticky=\"ew\", padx=5, pady=5)\n",
    "        \n",
    "        self.progress_bar = ttk.Progressbar(progress_frame, orient=\"horizontal\", mode=\"determinate\", variable=self.progress)\n",
    "        self.progress_bar.pack(fill=\"x\", expand=True)\n",
    "        \n",
    "        # Configurer le panneau droit (contrôles)\n",
    "        right_panel.columnconfigure(0, weight=1)\n",
    "        for i in range(10):  # Ajout d'une ligne pour le statut\n",
    "            right_panel.rowconfigure(i, weight=0)\n",
    "        right_panel.rowconfigure(10, weight=1)  # Pour la zone de résultats\n",
    "        \n",
    "        # Titre\n",
    "        title_label = ttk.Label(right_panel, text=\"Collecte de données VSR\", font=(\"Arial\", 16, \"bold\"))\n",
    "        title_label.grid(row=0, column=0, sticky=\"ew\", padx=5, pady=10)\n",
    "        \n",
    "        # Sélection du mot\n",
    "        word_frame = ttk.LabelFrame(right_panel, text=\"Mot à prononcer\")\n",
    "        word_frame.grid(row=1, column=0, sticky=\"ew\", padx=5, pady=5)\n",
    "        \n",
    "        word_display = ttk.Label(word_frame, textvariable=self.current_word, font=(\"Arial\", 40, \"bold\"))\n",
    "        word_display.pack(padx=20, pady=20)\n",
    "        \n",
    "        # Numéro d'exemple\n",
    "        example_frame = ttk.Frame(right_panel)\n",
    "        example_frame.grid(row=2, column=0, sticky=\"ew\", padx=5, pady=5)\n",
    "        \n",
    "        example_label = ttk.Label(example_frame, text=\"Exemple:\")\n",
    "        example_label.pack(side=\"left\", padx=5)\n",
    "        \n",
    "        example_counter = ttk.Label(example_frame, textvariable=self.current_example)\n",
    "        example_counter.pack(side=\"left\", padx=5)\n",
    "        \n",
    "        ttk.Label(example_frame, text=\"/\").pack(side=\"left\")\n",
    "        \n",
    "        ttk.Label(example_frame, text=str(examples_per_word)).pack(side=\"left\", padx=5)\n",
    "        \n",
    "        # Boutons de navigation\n",
    "        nav_frame = ttk.Frame(right_panel)\n",
    "        nav_frame.grid(row=3, column=0, sticky=\"ew\", padx=5, pady=5)\n",
    "        \n",
    "        prev_word_btn = ttk.Button(nav_frame, text=\"Mot précédent\", command=self.prev_word)\n",
    "        prev_word_btn.pack(side=\"left\", padx=5, expand=True)\n",
    "        \n",
    "        next_word_btn = ttk.Button(nav_frame, text=\"Mot suivant\", command=self.next_word)\n",
    "        next_word_btn.pack(side=\"left\", padx=5, expand=True)\n",
    "        \n",
    "        # Boutons d'enregistrement\n",
    "        record_frame = ttk.Frame(right_panel)\n",
    "        record_frame.grid(row=4, column=0, sticky=\"ew\", padx=5, pady=10)\n",
    "        \n",
    "        self.record_btn = ttk.Button(record_frame, text=\"Enregistrer\", command=self.start_recording)\n",
    "        self.record_btn.pack(fill=\"x\", padx=5, pady=5)\n",
    "        \n",
    "        # Compte à rebours\n",
    "        countdown_frame = ttk.Frame(right_panel)\n",
    "        countdown_frame.grid(row=5, column=0, sticky=\"ew\", padx=5, pady=5)\n",
    "        \n",
    "        self.countdown_label = ttk.Label(countdown_frame, textvariable=self.countdown_value, font=(\"Arial\", 30))\n",
    "        self.countdown_label.pack(padx=5, pady=5)\n",
    "        \n",
    "        # Qualité de l'enregistrement\n",
    "        quality_frame = ttk.LabelFrame(right_panel, text=\"Qualité de l'enregistrement\")\n",
    "        quality_frame.grid(row=6, column=0, sticky=\"ew\", padx=5, pady=5)\n",
    "        \n",
    "        quality_label = ttk.Label(quality_frame, textvariable=self.video_quality, font=(\"Arial\", 12))\n",
    "        quality_label.pack(padx=5, pady=5)\n",
    "        \n",
    "        # Bouton de replay\n",
    "        replay_frame = ttk.Frame(right_panel)\n",
    "        replay_frame.grid(row=7, column=0, sticky=\"ew\", padx=5, pady=5)\n",
    "        \n",
    "        self.replay_btn = ttk.Button(replay_frame, text=\"Rejouer avec son\", command=self.replay_recording, state=\"disabled\")\n",
    "        self.replay_btn.pack(fill=\"x\", padx=5, pady=5)\n",
    "        \n",
    "        # Boutons de validation\n",
    "        validation_frame = ttk.Frame(right_panel)\n",
    "        validation_frame.grid(row=8, column=0, sticky=\"ew\", padx=5, pady=5)\n",
    "        \n",
    "        accept_btn = ttk.Button(validation_frame, text=\"Accepter\", command=self.accept_recording)\n",
    "        accept_btn.pack(side=\"left\", padx=5, expand=True)\n",
    "        \n",
    "        reject_btn = ttk.Button(validation_frame, text=\"Rejeter\", command=self.reject_recording)\n",
    "        reject_btn.pack(side=\"left\", padx=5, expand=True)\n",
    "        \n",
    "        # Zone de statut (nouveau)\n",
    "        status_frame = ttk.Frame(right_panel)\n",
    "        status_frame.grid(row=9, column=0, sticky=\"ew\", padx=5, pady=5)\n",
    "        \n",
    "        status_label = ttk.Label(status_frame, textvariable=self.status_message, font=(\"Arial\", 10), wraplength=250)\n",
    "        status_label.pack(fill=\"x\", padx=5, pady=5)\n",
    "        \n",
    "        # Zone de résultats\n",
    "        results_frame = ttk.LabelFrame(right_panel, text=\"Résultats de détection\")\n",
    "        results_frame.grid(row=10, column=0, sticky=\"nsew\", padx=5, pady=5)\n",
    "        \n",
    "        self.lip_canvas = tk.Canvas(results_frame, bg=\"white\")\n",
    "        self.lip_canvas.pack(fill=\"both\", expand=True, padx=5, pady=5)\n",
    "    \n",
    "    def start_webcam(self):\n",
    "        self.cap = cv2.VideoCapture(0)\n",
    "        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, width)\n",
    "        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, height)\n",
    "        self.update_webcam()\n",
    "    \n",
    "    def update_webcam(self):\n",
    "        if self.cap is not None and self.cap.isOpened():\n",
    "            ret, frame = self.cap.read()\n",
    "            if ret:\n",
    "                # Dessiner un rectangle au milieu pour guider l'utilisateur\n",
    "                h, w = frame.shape[:2]\n",
    "                center_x, center_y = w // 2, h // 2\n",
    "                rect_w, rect_h = 300, 200\n",
    "                cv2.rectangle(frame, \n",
    "                             (center_x - rect_w // 2, center_y - rect_h // 2),\n",
    "                             (center_x + rect_w // 2, center_y + rect_h // 2),\n",
    "                             (0, 255, 0), 2)\n",
    "                \n",
    "                # Extraction des lèvres pour le retour en direct (facultatif)\n",
    "                if not self.is_recording:\n",
    "                    lip_region, lips_points, face = extract_lip_region(frame)\n",
    "                    if face is not None and lips_points is not None:\n",
    "                        # Dessiner les points des lèvres\n",
    "                        for (x, y) in lips_points:\n",
    "                            cv2.circle(frame, (x, y), 2, (0, 0, 255), -1)\n",
    "                \n",
    "                # Convertir l'image pour l'affichage Tkinter\n",
    "                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "                img = Image.fromarray(frame_rgb)\n",
    "                imgtk = ImageTk.PhotoImage(image=img)\n",
    "                \n",
    "                # Mettre à jour l'affichage\n",
    "                self.webcam_label.imgtk = imgtk\n",
    "                self.webcam_label.configure(image=imgtk)\n",
    "            \n",
    "            # Mettre à jour périodiquement\n",
    "            self.root.after(15, self.update_webcam)\n",
    "    \n",
    "    def prev_word(self):\n",
    "        if not self.is_recording:\n",
    "            current_idx = target_words.index(self.current_word.get())\n",
    "            if current_idx > 0:\n",
    "                self.current_word.set(target_words[current_idx - 1])\n",
    "                self.current_example.set(1)\n",
    "    \n",
    "    def next_word(self):\n",
    "        if not self.is_recording:\n",
    "            current_idx = target_words.index(self.current_word.get())\n",
    "            if current_idx < len(target_words) - 1:\n",
    "                self.current_word.set(target_words[current_idx + 1])\n",
    "                self.current_example.set(1)\n",
    "    \n",
    "    def start_recording(self):\n",
    "        if not self.is_recording and self.cap is not None:\n",
    "            self.record_btn.configure(state=\"disabled\")\n",
    "            self.replay_btn.configure(state=\"disabled\")\n",
    "            self.is_recording = True\n",
    "            self.frames = []\n",
    "            self.lip_regions = []\n",
    "            self.audio_data = None\n",
    "            self.progress.set(0)\n",
    "            self.status_message.set(\"\")  # Effacer le message de statut\n",
    "            \n",
    "            # Nettoyer les fichiers temporaires précédents\n",
    "            self.cleanup_temp_files()\n",
    "            \n",
    "            # Démarrer l'enregistrement dans un thread séparé\n",
    "            self.recording_thread = threading.Thread(target=self.recording_process)\n",
    "            self.recording_thread.daemon = True\n",
    "            self.recording_thread.start()\n",
    "    \n",
    "    def recording_process(self):\n",
    "        # Initialiser le compte à rebours\n",
    "        for i in range(3, 0, -1):\n",
    "            self.countdown_value.set(i)\n",
    "            self.root.update()\n",
    "            time.sleep(0.7)\n",
    "        \n",
    "        self.countdown_value.set(\"GO!\")\n",
    "        self.root.update()\n",
    "        \n",
    "        # Configurer l'enregistrement audio\n",
    "        audio_frames = []\n",
    "        \n",
    "        # Fonction de callback pour l'enregistrement audio\n",
    "        def audio_callback(indata, frames, time, status):\n",
    "            if status:\n",
    "                print(f\"Erreur audio: {status}\")\n",
    "            audio_frames.append(indata.copy())\n",
    "        \n",
    "        # Démarrer l'enregistrement audio dans un thread\n",
    "        audio_stream = sd.InputStream(samplerate=audio_fs, channels=1, callback=audio_callback)\n",
    "        audio_stream.start()\n",
    "        \n",
    "        # Enregistrement vidéo\n",
    "        start_time = time.time()\n",
    "        frame_count = 0\n",
    "        max_frames = int(duration * frame_rate)\n",
    "        \n",
    "        while time.time() - start_time < duration:\n",
    "            if self.cap is not None:\n",
    "                ret, frame = self.cap.read()\n",
    "                if ret:\n",
    "                    self.frames.append(frame)\n",
    "                    \n",
    "                    # Extraire et sauvegarder la région des lèvres\n",
    "                    lip_region, _, _ = extract_lip_region(frame)\n",
    "                    if lip_region is not None:\n",
    "                        self.lip_regions.append(lip_region)\n",
    "                    \n",
    "                    # Mise à jour de la progression\n",
    "                    frame_count += 1\n",
    "                    progress_percent = min(100, (frame_count / max_frames) * 100)\n",
    "                    self.progress.set(progress_percent)\n",
    "                    \n",
    "                    # Petite pause pour ne pas surcharger\n",
    "                    time.sleep(0.01)\n",
    "        \n",
    "        # Arrêter l'enregistrement audio\n",
    "        audio_stream.stop()\n",
    "        audio_stream.close()\n",
    "        \n",
    "        # Convertir et sauvegarder l'audio dans un fichier temporaire\n",
    "        if audio_frames:\n",
    "            audio_data = np.vstack(audio_frames)\n",
    "            self.audio_data = audio_data.flatten()\n",
    "            \n",
    "            # Créer un fichier temporaire pour l'audio\n",
    "            fd, temp_path = tempfile.mkstemp(suffix='.wav')\n",
    "            os.close(fd)\n",
    "            self.temp_audio_file = temp_path\n",
    "            sf.write(self.temp_audio_file, self.audio_data, audio_fs)\n",
    "        \n",
    "        # Sauvegarder la vidéo dans un fichier temporaire pour le replay\n",
    "        if self.frames:\n",
    "            fd, temp_path = tempfile.mkstemp(suffix='.mp4')\n",
    "            os.close(fd)\n",
    "            self.temp_video_file = temp_path\n",
    "            \n",
    "            # Taille de la première image\n",
    "            h, w = self.frames[0].shape[:2]\n",
    "            \n",
    "            # Créer l'objet VideoWriter\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "            out = cv2.VideoWriter(self.temp_video_file, fourcc, frame_rate, (w, h))\n",
    "            \n",
    "            # Écrire chaque image dans le fichier vidéo\n",
    "            for frame in self.frames:\n",
    "                out.write(frame)\n",
    "            \n",
    "            # Libérer les ressources\n",
    "            out.release()\n",
    "        \n",
    "        # Terminer l'enregistrement\n",
    "        self.countdown_value.set(\"Fin\")\n",
    "        self.is_recording = False\n",
    "        \n",
    "        # Analyser la qualité de l'enregistrement\n",
    "        self.analyze_recording()\n",
    "        \n",
    "        # Réactiver les boutons\n",
    "        self.record_btn.configure(state=\"normal\")\n",
    "        if self.temp_audio_file and self.temp_video_file:\n",
    "            self.replay_btn.configure(state=\"normal\")\n",
    "            self.status_message.set(\"Enregistrement terminé. Vous pouvez rejouer, accepter ou rejeter.\")\n",
    "    \n",
    "    def replay_recording(self):\n",
    "        \"\"\"\n",
    "        Rejoue l'enregistrement avec le son\n",
    "        \"\"\"\n",
    "        if not self.temp_audio_file:\n",
    "            self.status_message.set(\"Pas d'audio disponible pour le replay!\")\n",
    "            return\n",
    "        \n",
    "        # Lecture du son\n",
    "        try:\n",
    "            pygame.mixer.music.load(self.temp_audio_file)\n",
    "            pygame.mixer.music.play()\n",
    "            \n",
    "            # Afficher des informations dans la zone de statut\n",
    "            self.status_message.set(\"Lecture de l'audio en cours...\")\n",
    "            \n",
    "            # Désactiver les boutons pendant la lecture\n",
    "            self.replay_btn.configure(state=\"disabled\")\n",
    "            \n",
    "            # Vérifier périodiquement si la lecture est terminée\n",
    "            def check_playback_done():\n",
    "                if pygame.mixer.music.get_busy():\n",
    "                    self.root.after(100, check_playback_done)\n",
    "                else:\n",
    "                    self.replay_btn.configure(state=\"normal\")\n",
    "                    self.status_message.set(\"Lecture terminée\")\n",
    "            \n",
    "            check_playback_done()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.status_message.set(f\"Erreur lors de la lecture: {e}\")\n",
    "            self.replay_btn.configure(state=\"normal\")\n",
    "    \n",
    "    def cleanup_temp_files(self):\n",
    "        \"\"\"\n",
    "        Nettoie les fichiers temporaires\n",
    "        \"\"\"\n",
    "        if self.temp_audio_file and os.path.exists(self.temp_audio_file):\n",
    "            try:\n",
    "                os.remove(self.temp_audio_file)\n",
    "            except:\n",
    "                pass\n",
    "            self.temp_audio_file = None\n",
    "        \n",
    "        if self.temp_video_file and os.path.exists(self.temp_video_file):\n",
    "            try:\n",
    "                os.remove(self.temp_video_file)\n",
    "            except:\n",
    "                pass\n",
    "            self.temp_video_file = None\n",
    "    \n",
    "    def analyze_recording(self):\n",
    "        if not self.frames:\n",
    "            self.video_quality.set(\"Aucune image capturée!\")\n",
    "            return\n",
    "        \n",
    "        # Analyse simple: vérifier que des lèvres ont été détectées\n",
    "        if len(self.lip_regions) < len(self.frames) * 0.7:\n",
    "            self.video_quality.set(\"Mauvais: Lèvres détectées dans moins de 70% des images\")\n",
    "        elif len(self.lip_regions) < len(self.frames) * 0.9:\n",
    "            self.video_quality.set(\"Acceptable: Lèvres détectées dans plus de 70% des images\")\n",
    "        else:\n",
    "            self.video_quality.set(\"Excellent: Lèvres détectées dans plus de 90% des images\")\n",
    "        \n",
    "        # Afficher quelques images des lèvres dans le canvas\n",
    "        self.show_lip_results()\n",
    "    \n",
    "    def show_lip_results(self):\n",
    "        if not self.lip_regions:\n",
    "            return\n",
    "        \n",
    "        # Effacer le canvas\n",
    "        self.lip_canvas.delete(\"all\")\n",
    "        \n",
    "        # Sélectionner quelques images à afficher\n",
    "        num_display = min(6, len(self.lip_regions))\n",
    "        display_indices = np.linspace(0, len(self.lip_regions)-1, num_display, dtype=int)\n",
    "        \n",
    "        # Calculer la taille et la position des images\n",
    "        canvas_width = self.lip_canvas.winfo_width()\n",
    "        canvas_height = self.lip_canvas.winfo_height()\n",
    "        \n",
    "        if canvas_width <= 1 or canvas_height <= 1:\n",
    "            # Le canvas n'est pas encore initialisé, attendre et réessayer\n",
    "            self.root.after(100, self.show_lip_results)\n",
    "            return\n",
    "        \n",
    "        img_width = (canvas_width - 20) // 3\n",
    "        img_height = (canvas_height - 20) // 2\n",
    "        \n",
    "        for i, idx in enumerate(display_indices):\n",
    "            # Position dans une grille 3x2\n",
    "            row = i // 3\n",
    "            col = i % 3\n",
    "            x = 10 + col * img_width\n",
    "            y = 10 + row * img_height\n",
    "            \n",
    "            # Obtenir et redimensionner l'image des lèvres\n",
    "            lip_img = self.lip_regions[idx].copy()\n",
    "            lip_img = cv2.resize(lip_img, (img_width - 10, img_height - 10))\n",
    "            lip_img_rgb = cv2.cvtColor(lip_img, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Convertir en format Tkinter\n",
    "            pil_img = Image.fromarray(lip_img_rgb)\n",
    "            tk_img = ImageTk.PhotoImage(pil_img)\n",
    "            \n",
    "            # Sauvegarder la référence (important pour éviter le garbage collection)\n",
    "            self.lip_canvas.imgs = getattr(self.lip_canvas, 'imgs', [])\n",
    "            self.lip_canvas.imgs.append(tk_img)\n",
    "            \n",
    "            # Afficher l'image\n",
    "            self.lip_canvas.create_image(x, y, anchor=\"nw\", image=tk_img)\n",
    "            self.lip_canvas.create_text(x + img_width//2, y + img_height - 10, \n",
    "                                       text=f\"Frame {idx}\", fill=\"black\")\n",
    "    \n",
    "    def accept_recording(self):\n",
    "        if not self.frames or not self.lip_regions:\n",
    "            self.status_message.set(\"Aucun enregistrement à sauvegarder!\")\n",
    "            return\n",
    "        \n",
    "        word = self.current_word.get()\n",
    "        example_idx = self.current_example.get()\n",
    "        \n",
    "        # Sauvegarder la vidéo\n",
    "        video_filename = f\"{word}_{example_idx:02d}.mp4\"\n",
    "        video_path = os.path.join(video_dir, word, video_filename)\n",
    "        \n",
    "        # Taille de la première image\n",
    "        h, w = self.frames[0].shape[:2]\n",
    "        \n",
    "        # Créer l'objet VideoWriter\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(video_path, fourcc, frame_rate, (w, h))\n",
    "        \n",
    "        # Écrire chaque image dans le fichier vidéo\n",
    "        for frame in self.frames:\n",
    "            out.write(frame)\n",
    "        \n",
    "        # Libérer les ressources\n",
    "        out.release()\n",
    "        \n",
    "        # Sauvegarder l'audio\n",
    "        if self.audio_data is not None:\n",
    "            audio_filename = f\"{word}_{example_idx:02d}.wav\"\n",
    "            audio_path = os.path.join(audio_dir, word, audio_filename)\n",
    "            sf.write(audio_path, self.audio_data, audio_fs)\n",
    "        \n",
    "        # Sauvegarder les régions des lèvres\n",
    "        lip_count = 0\n",
    "        for i, lip_region in enumerate(self.lip_regions):\n",
    "            lip_filename = f\"{word}_{example_idx:02d}_{i+1:03d}.png\"\n",
    "            lip_path = os.path.join(lip_dir, word, lip_filename)\n",
    "            cv2.imwrite(lip_path, lip_region)\n",
    "            lip_count += 1\n",
    "        \n",
    "        # Afficher un message dans la zone de statut au lieu d'une boîte de dialogue\n",
    "        self.status_message.set(f\"Enregistrement sauvegardé. {lip_count} images de lèvres sauvegardées.\")\n",
    "        \n",
    "        # Nettoyer les fichiers temporaires\n",
    "        self.cleanup_temp_files()\n",
    "        \n",
    "        # Passer à l'exemple suivant\n",
    "        if example_idx < examples_per_word:\n",
    "            self.current_example.set(example_idx + 1)\n",
    "        else:\n",
    "            # Passer au mot suivant si tous les exemples sont faits\n",
    "            current_word_idx = target_words.index(word)\n",
    "            if current_word_idx < len(target_words) - 1:\n",
    "                self.current_word.set(target_words[current_word_idx + 1])\n",
    "                self.current_example.set(1)\n",
    "            else:\n",
    "                self.status_message.set(\"Tous les mots et exemples ont été enregistrés!\")\n",
    "        \n",
    "        # Réinitialiser l'interface\n",
    "        self.video_quality.set(\"\")\n",
    "        self.countdown_value.set(3)\n",
    "        self.lip_canvas.delete(\"all\")\n",
    "        self.replay_btn.configure(state=\"disabled\")\n",
    "    \n",
    "    def reject_recording(self):\n",
    "        # Réinitialiser sans sauvegarder\n",
    "        self.frames = []\n",
    "        self.lip_regions = []\n",
    "        self.audio_data = None\n",
    "        self.video_quality.set(\"\")\n",
    "        self.countdown_value.set(3)\n",
    "        self.lip_canvas.delete(\"all\")\n",
    "        self.replay_btn.configure(state=\"disabled\")\n",
    "        self.status_message.set(\"Enregistrement rejeté.\")\n",
    "        \n",
    "        # Nettoyer les fichiers temporaires\n",
    "        self.cleanup_temp_files()\n",
    "    \n",
    "    def on_closing(self):\n",
    "        # Nettoyer les fichiers temporaires\n",
    "        self.cleanup_temp_files()\n",
    "        \n",
    "        # Libérer les ressources\n",
    "        if self.cap is not None:\n",
    "            self.cap.release()\n",
    "        \n",
    "        # Fermer pygame\n",
    "        pygame.mixer.quit()\n",
    "        \n",
    "        # Fermer l'application\n",
    "        self.root.destroy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fae40e8f-d9d0-4692-be96-973da54b4ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Vérifier le détecteur\n",
    "    if not os.path.exists(predictor_path):\n",
    "        print(\"ERREUR: Le fichier de points de repère n'a pas été trouvé !\")\n",
    "        print(\"Téléchargez-le depuis http://dlib.net/files/shape_predictor_68_face_landmarks.dat.bz2\")\n",
    "        print(\"Puis décompressez-le et placez-le dans le dossier 'models'\")\n",
    "        exit(1)\n",
    "    \n",
    "    # Créer l'application\n",
    "    root = tk.Tk()\n",
    "    app = VSRDataCollector(root)\n",
    "    root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5494545-e799-43fc-93fc-ce8f65a99912",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
